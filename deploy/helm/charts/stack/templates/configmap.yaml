apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "my-llama-stack.fullname" . }}
  labels:
    {{- include "my-llama-stack.labels" . | nindent 4 }}
immutable: false
data:
  INFERENCE_MODEL: "gpt-4o-mini"
  EMBEDDING_MODEL: "sentence-transformers/all-mpnet-base-v2"
  OLLAMA_URL: "http://envision.home.glroland.com:11434"
  MILVUS_ENDPOINT: "http://db.home.glroland.com:19530"
  MILVUS_TOKEN: "root:Milvus"
  LLAMA_STACK_LOGGING: "all=info"
  OTEL_EXPORTER_OTLP_ENDPOINT: "http://eclipse.home.glroland.com:4318"
  POSTGRES_HOST: "db.home.glroland.com"
  POSTGRES_PORT: "5432"
  POSTGRES_DB: "llamastack"
  POSTGRES_USER: "llamastack"
  POSTGRES_PASSWORD: "llamastack"

  run.yaml: |-
    version: '2'
    image_name: my-llama-stack
    container_image: my-llama-stack
    apis:
    - inference
    - vector_io
    - safety
    - agents
    - telemetry
    - eval
    - datasetio
    - scoring
    - tool_runtime
    providers:
      inference:
      #- provider_id: ollama
      #  provider_type: remote::ollama
      #  config:
      #    url: ${env.OLLAMA_URL:=http://localhost:11434}
      - provider_id: sentence-transformers
        provider_type: inline::sentence-transformers
        config: {}
      - provider_id: openai
        provider_type: remote::openai
        config:
          api_key: ${env.OPENAI_API_KEY:=nokeyneeded}
      - provider_id: together
        provider_type: remote::together
        config:
          url: https://api.together.xyz/v1
          api_key: ${env.TOGETHER_API_KEY:=null}
      vector_io:
      - provider_id: faiss
        provider_type: inline::faiss
        config:
          kvstore:
            type: postgres
            host: ${env.POSTGRES_HOST:=localhost}
            port: ${env.POSTGRES_PORT:=5432}
            db: ${env.POSTGRES_DB:=llamastack}
            user: ${env.POSTGRES_USER:=llamastack}
            password: ${env.POSTGRES_PASSWORD:=llamastack}
            table_name: ${env.POSTGRES_TABLE_NAME:=vectorio_faiss_kvstore}
      # ERROR    2025-06-21 23:36:18,716 llama_stack.distribution.resolver:394 core: Method openai_list_files_in_vector_store incompatible proto: {'vector_store_id', 'after', 'limit', 'filter', 'before', 'order'} vs. obj: {'vector_store_id'} 
      # ValueError: Provider `milvus (Api.vector_io)` does not implement the following methods:[('openai_list_files_in_vector_store', 'signature_mismatch')]
      # 
      #- provider_id: milvus
      #  provider_type: remote::milvus
      #  config:
      #    uri: ${env.MILVUS_ENDPOINT}
      #    token: ${env.MILVUS_TOKEN}
      files:
      - provider_id: meta-reference-files
        provider_type: inline::localfs
        config:
          storage_dir: ${env.FILES_STORAGE_DIR:=~/.llama/distributions/starter/files}
          metadata_store:
            type: postgres
            host: ${env.POSTGRES_HOST:=localhost}
            port: ${env.POSTGRES_PORT:=5432}
            db: ${env.POSTGRES_DB:=llamastack}
            user: ${env.POSTGRES_USER:=llamastack}
            password: ${env.POSTGRES_PASSWORD:=llamastack}
            table_name: ${env.POSTGRES_TABLE_NAME:=openai_files}
      safety:
      - provider_id: llama-guard
        provider_type: inline::llama-guard
        config:
          excluded_categories: []
      - provider_id: code-scanner
        provider_type: inline::code-scanner
      agents:
      - provider_id: meta-reference
        provider_type: inline::meta-reference
        config:
          persistence_store:
            type: postgres
            host: ${env.POSTGRES_HOST:=localhost}
            port: ${env.POSTGRES_PORT:=5432}
            db: ${env.POSTGRES_DB:=llamastack}
            user: ${env.POSTGRES_USER:=llamastack}
            password: ${env.POSTGRES_PASSWORD:=llamastack}
            table_name: ${env.POSTGRES_TABLE_NAME:=agents_store}
          responses_store:
            type: postgres
            host: ${env.POSTGRES_HOST:=localhost}
            port: ${env.POSTGRES_PORT:=5432}
            db: ${env.POSTGRES_DB:=llamastack}
            user: ${env.POSTGRES_USER:=llamastack}
            password: ${env.POSTGRES_PASSWORD:=llamastack}
            table_name: ${env.POSTGRES_TABLE_NAME:=openai_responses}
      telemetry:
      - provider_id: meta-reference
        provider_type: inline::meta-reference
        config:
          service_name: "${env.OTEL_SERVICE_NAME:=my-llama-stack}"
          sinks: ${env.TELEMETRY_SINKS:=otel_trace,otel_metric,console}
          otel_exporter_otlp_endpoint: "${env.OTEL_EXPORTER_OTLP_ENDPOINT:=}"
      post_training:
      - provider_id: torchtune-cpu
        provider_type: inline::torchtune-cpu
        config:
          checkpoint_format: meta
      eval:
      - provider_id: meta-reference
        provider_type: inline::meta-reference
        config:
          kvstore:
            type: postgres
            host: ${env.POSTGRES_HOST:=localhost}
            port: ${env.POSTGRES_PORT:=5432}
            db: ${env.POSTGRES_DB:=llamastack}
            user: ${env.POSTGRES_USER:=llamastack}
            password: ${env.POSTGRES_PASSWORD:=llamastack}
            table_name: ${env.POSTGRES_TABLE_NAME:=eval_kvstore}
      datasetio:
      - provider_id: huggingface
        provider_type: remote::huggingface
        config:
          kvstore:
            type: postgres
            host: ${env.POSTGRES_HOST:=localhost}
            port: ${env.POSTGRES_PORT:=5432}
            db: ${env.POSTGRES_DB:=llamastack}
            user: ${env.POSTGRES_USER:=llamastack}
            password: ${env.POSTGRES_PASSWORD:=llamastack}
            table_name: ${env.POSTGRES_TABLE_NAME:=datasetio_hf_kvstore}
      - provider_id: localfs
        provider_type: inline::localfs
        config:
          kvstore:
            type: postgres
            host: ${env.POSTGRES_HOST:=localhost}
            port: ${env.POSTGRES_PORT:=5432}
            db: ${env.POSTGRES_DB:=llamastack}
            user: ${env.POSTGRES_USER:=llamastack}
            password: ${env.POSTGRES_PASSWORD:=llamastack}
            table_name: ${env.POSTGRES_TABLE_NAME:=datasetio_localfs_kvstore}
      scoring:
      - provider_id: basic
        provider_type: inline::basic
        config: {}
      - provider_id: llm-as-judge
        provider_type: inline::llm-as-judge
        config: {}
      - provider_id: braintrust
        provider_type: inline::braintrust
        config:
          openai_api_key: ${env.OPENAI_API_KEY:=null}
      tool_runtime:
      - provider_id: tavily-search
        provider_type: remote::tavily-search
        config:
          api_key: ${env.TAVILY_SEARCH_API_KEY:=null}
          max_results: 3
      - provider_id: rag-runtime
        provider_type: inline::rag-runtime
        config: {}
      - provider_id: model-context-protocol
        provider_type: remote::model-context-protocol
        config: {}
    metadata_store:
      type: postgres
      host: ${env.POSTGRES_HOST:=localhost}
      port: ${env.POSTGRES_PORT:=5432}
      db: ${env.POSTGRES_DB:=llamastack}
      user: ${env.POSTGRES_USER:=llamastack}
      password: ${env.POSTGRES_PASSWORD:=llamastack}
      table_name: ${env.POSTGRES_TABLE_NAME:=metadata_store}
    inference_store:
      type: postgres
      host: ${env.POSTGRES_HOST:=localhost}
      port: ${env.POSTGRES_PORT:=5432}
      db: ${env.POSTGRES_DB:=llamastack}
      user: ${env.POSTGRES_USER:=llamastack}
      password: ${env.POSTGRES_PASSWORD:=llamastack}
      table_name: ${env.POSTGRES_TABLE_NAME:=inference_store}
    batches:
    - provider_id: reference
      provider_type: inline::reference
      config:
        kvstore:
          type: postgres
          host: ${env.POSTGRES_HOST:=localhost}
          port: ${env.POSTGRES_PORT:=5432}
          db: ${env.POSTGRES_DB:=llamastack}
          user: ${env.POSTGRES_USER:=llamastack}
          password: ${env.POSTGRES_PASSWORD:=llamastack}
          table_name: ${env.POSTGRES_TABLE_NAME:=batches_kvstore}
    models:
    #- metadata: {}
    #  model_id: ${env.INFERENCE_MODEL:=null}
    #  provider_id: ollama
    #  model_type: llm
    - metadata:
        embedding_dimension: 3072
      model_id: text-embedding-3-large
      provider_id: openai
      provider_model_id: text-embedding-3-large
      model_type: embedding
    - metadata: {}
      model_id: gpt-4o-mini
      provider_id: openai
      model_type: llm
    - metadata: {}
      model_id: meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo
      provider_id: together
      provider_model_id: meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo
      model_type: llm
    - metadata: {}
      model_id: meta-llama/Llama-3.1-8B-Instruct
      provider_id: together
      provider_model_id: meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo
      model_type: llm
    - metadata: {}
      model_id: meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo
      provider_id: together
      provider_model_id: meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo
      model_type: llm
    - metadata: {}
      model_id: meta-llama/Llama-3.1-70B-Instruct
      provider_id: together
      provider_model_id: meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo
      model_type: llm
    - metadata: {}
      model_id: meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo
      provider_id: together
      provider_model_id: meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo
      model_type: llm
    - metadata: {}
      model_id: meta-llama/Llama-3.1-405B-Instruct-FP8
      provider_id: together
      provider_model_id: meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo
      model_type: llm
    - metadata: {}
      model_id: meta-llama/Llama-3.2-3B-Instruct-Turbo
      provider_id: together
      provider_model_id: meta-llama/Llama-3.2-3B-Instruct-Turbo
      model_type: llm
    - metadata: {}
      model_id: meta-llama/Llama-3.2-3B-Instruct
      provider_id: together
      provider_model_id: meta-llama/Llama-3.2-3B-Instruct-Turbo
      model_type: llm
    - metadata: {}
      model_id: meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo
      provider_id: together
      provider_model_id: meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo
      model_type: llm
    - metadata: {}
      model_id: meta-llama/Llama-3.2-11B-Vision-Instruct
      provider_id: together
      provider_model_id: meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo
      model_type: llm
    - metadata: {}
      model_id: meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo
      provider_id: together
      provider_model_id: meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo
      model_type: llm
    - metadata: {}
      model_id: meta-llama/Llama-3.2-90B-Vision-Instruct
      provider_id: together
      provider_model_id: meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo
      model_type: llm
    - metadata: {}
      model_id: meta-llama/Llama-3.3-70B-Instruct-Turbo
      provider_id: together
      provider_model_id: meta-llama/Llama-3.3-70B-Instruct-Turbo
      model_type: llm
    - metadata: {}
      model_id: meta-llama/Llama-3.3-70B-Instruct
      provider_id: together
      provider_model_id: meta-llama/Llama-3.3-70B-Instruct-Turbo
      model_type: llm
    - metadata: {}
      model_id: meta-llama/Llama-Guard-3-8B
      provider_id: together
      provider_model_id: meta-llama/Llama-Guard-3-8B
      model_type: llm
    - metadata: {}
      model_id: meta-llama/Llama-Guard-3-11B-Vision-Turbo
      provider_id: together
      provider_model_id: meta-llama/Llama-Guard-3-11B-Vision-Turbo
      model_type: llm
    - metadata: {}
      model_id: meta-llama/Llama-Guard-3-11B-Vision
      provider_id: together
      provider_model_id: meta-llama/Llama-Guard-3-11B-Vision-Turbo
      model_type: llm
    - metadata:
        embedding_dimension: 768
        context_length: 8192
      model_id: togethercomputer/m2-bert-80M-8k-retrieval
      provider_id: together
      provider_model_id: togethercomputer/m2-bert-80M-8k-retrieval
      model_type: embedding
    - metadata:
        embedding_dimension: 768
        context_length: 32768
      model_id: togethercomputer/m2-bert-80M-32k-retrieval
      provider_id: together
      provider_model_id: togethercomputer/m2-bert-80M-32k-retrieval
      model_type: embedding
    - metadata: {}
      model_id: meta-llama/Llama-4-Scout-17B-16E-Instruct
      provider_id: together
      provider_model_id: meta-llama/Llama-4-Scout-17B-16E-Instruct
      model_type: llm
    - metadata: {}
      model_id: meta-llama/Llama-4-Scout-17B-16E-Instruct
      provider_id: together
      provider_model_id: meta-llama/Llama-4-Scout-17B-16E-Instruct
      model_type: llm
    - metadata: {}
      model_id: together/meta-llama/Llama-4-Scout-17B-16E-Instruct
      provider_id: together
      provider_model_id: meta-llama/Llama-4-Scout-17B-16E-Instruct
      model_type: llm
    - metadata: {}
      model_id: meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8
      provider_id: together
      provider_model_id: meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8
      model_type: llm
    - metadata: {}
      model_id: meta-llama/Llama-4-Maverick-17B-128E-Instruct
      provider_id: together
      provider_model_id: meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8
      model_type: llm
    - metadata: {}
      model_id: together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8
      provider_id: together
      provider_model_id: meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8
      model_type: llm
    shields: []
    vector_dbs: []
    datasets: []
    scoring_fns: []
    benchmarks: []
    tool_groups:
    - toolgroup_id: builtin::websearch
      provider_id: tavily-search
    - toolgroup_id: builtin::rag
      provider_id: rag-runtime
    #- toolgroup_id: builtin::code_interpreter
    #  provider_id: code-interpreter

    - toolgroup_id: mcp::ansible-mcp
      provider_id: model-context-protocol
      mcp_endpoint: {uri: https://ansible-mcp-ansible-mcp.apps.ocp.home.glroland.com/sse}

    - toolgroup_id: mcp::agent-utilities
      provider_id: model-context-protocol
      mcp_endpoint: {uri: https://baseball-chatbot-agent-utilities-baseball-chatbot.apps.ocp.home.glroland.com/sse}

    - toolgroup_id: mcp::agent-game
      provider_id: model-context-protocol
      mcp_endpoint: {uri: https://baseball-chatbot-agent-game-baseball-chatbot.apps.ocp.home.glroland.com/sse}

    - toolgroup_id: mcp::agent-team
      provider_id: model-context-protocol
      mcp_endpoint: {uri: https://baseball-chatbot-agent-team-baseball-chatbot.apps.ocp.home.glroland.com/sse}

    logging: null
    server:
      port: {{ .Values.image.port }}
